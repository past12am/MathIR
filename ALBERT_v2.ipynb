{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V70ynnaNYNnD",
        "outputId": "27ac12ae-9114-4dd3-f1f7-6f0f3bfebc62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylatexenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywQebEVbYAWX",
        "outputId": "abe6c1af-e818-4bea-fc68-08b81ce0f899"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pylatexenc\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/162.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/162.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=0654327c92133e4e995a6ff0decb2d74441f9d850e91eb2716da21f81ad2f1bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc\n",
            "Successfully installed pylatexenc-2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnO7rqabzXY4",
        "outputId": "be66bcb2-0b8f-4ed4-b0cc-5a9ad70b90c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import join\n",
        "dirfiles = listdir(\"/content/drive/My Drive/MathIRData/ARQMathAgg/dataset_v2/\")\n",
        "print(dirfiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMuPXKnUzZby",
        "outputId": "e75bd66e-cd1e-4315-f5be-e3f8b4a3315a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['queries_train.tsv', 'collection_train.tsv', 'triples_train.jsonl', 'qrel_train', 'queries_test.tsv', 'qrel_test', 'triples_test.jsonl', '.gitignore', 'collection_test.tsv', 'aggregates']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ir-measures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUuQtPMw5xE7",
        "outputId": "1bfccadd-25f4-41a7-d7a0-459963db09de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ir-measures in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: pytrec-eval-terrier>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from ir-measures) (0.5.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vWow2o61wNg",
        "outputId": "cb923471-a2fc-4c6a-a180-43742401d882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['queries_train.tsv', 'collection_train.tsv', 'triples_train.jsonl', 'qrel_train', 'queries_test.tsv', 'qrel_test', 'triples_test.jsonl', '.gitignore', 'collection_test.tsv', 'aggregates']\n",
            "0/99\n",
            "2461992 Q0 0 1 0.9997264742851257 STANDARD\n",
            "\n",
            "1/99\n",
            "2309227 Q0 2 1 0.9997496008872986 STANDARD\n",
            "\n",
            "2/99\n",
            "2017015 Q0 3 1 0.9995967745780945 STANDARD\n",
            "\n",
            "3/99\n",
            "955328 Q0 4 1 0.9997803568840027 STANDARD\n",
            "\n",
            "4/99\n",
            "1562252 Q0 7 1 0.9995365142822266 STANDARD\n",
            "\n",
            "5/99\n",
            "2416868 Q0 9 1 0.839702844619751 STANDARD\n",
            "\n",
            "6/99\n",
            "2103852 Q0 10 1 0.9997084736824036 STANDARD\n",
            "\n",
            "7/99\n",
            "2212375 Q0 12 1 0.9992941617965698 STANDARD\n",
            "\n",
            "8/99\n",
            "2599812 Q0 15 1 0.9997578263282776 STANDARD\n",
            "\n",
            "9/99\n",
            "384581 Q0 17 1 0.9997630715370178 STANDARD\n",
            "\n",
            "10/99\n",
            "164534 Q0 18 1 0.9983805418014526 STANDARD\n",
            "\n",
            "11/99\n",
            "3050853 Q0 20 1 0.9996416568756104 STANDARD\n",
            "\n",
            "12/99\n",
            "94359 Q0 22 1 0.9728226661682129 STANDARD\n",
            "\n",
            "13/99\n",
            "2322942 Q0 30 1 0.9997305274009705 STANDARD\n",
            "\n",
            "14/99\n",
            "1554993 Q0 31 1 0.9997138381004333 STANDARD\n",
            "\n",
            "15/99\n",
            "462006 Q0 33 1 0.9996705055236816 STANDARD\n",
            "\n",
            "16/99\n",
            "773547 Q0 34 1 0.9997046589851379 STANDARD\n",
            "\n",
            "17/99\n",
            "2469966 Q0 35 1 0.9996871948242188 STANDARD\n",
            "\n",
            "18/99\n",
            "1309780 Q0 36 1 0.9993717074394226 STANDARD\n",
            "\n",
            "19/99\n",
            "2621850 Q0 41 1 0.9996888637542725 STANDARD\n",
            "\n",
            "20/99\n",
            "2480837 Q0 43 1 0.24350234866142273 STANDARD\n",
            "\n",
            "21/99\n",
            "1564176 Q0 46 1 0.991209864616394 STANDARD\n",
            "\n",
            "22/99\n",
            "2375401 Q0 48 1 0.9829962849617004 STANDARD\n",
            "\n",
            "23/99\n",
            "2296422 Q0 49 1 0.9996771812438965 STANDARD\n",
            "\n",
            "24/99\n",
            "1781009 Q0 50 1 0.9996215105056763 STANDARD\n",
            "\n",
            "25/99\n",
            "2832373 Q0 52 1 0.9997166991233826 STANDARD\n",
            "\n",
            "26/99\n",
            "742336 Q0 53 1 0.9964355230331421 STANDARD\n",
            "\n",
            "27/99\n",
            "2296566 Q0 54 1 0.973314106464386 STANDARD\n",
            "\n",
            "28/99\n",
            "9372 Q0 55 1 0.9994139671325684 STANDARD\n",
            "\n",
            "29/99\n",
            "1189145 Q0 61 1 0.9992478489875793 STANDARD\n",
            "\n",
            "30/99\n",
            "1187996 Q0 63 1 0.999769389629364 STANDARD\n",
            "\n",
            "Evaluating on:\n",
            "[nDCG@1, P@1, Judged@1, RR@1, AP@1]\n",
            "Metric,Value\n",
            "\n",
            "nDCG@1,0.00011911394593763808\n",
            "\n",
            "P@1,0.00011911394593763808\n",
            "\n",
            "Judged@1,0.00011911394593763808\n",
            "\n",
            "RR@1,0.00011911394593763808\n",
            "\n",
            "AP@1,8.065807253142753e-05\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "import re\n",
        "import ir_measures\n",
        "from ir_measures import nDCG, P, Judged, MRR, MAP, R\n",
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def main():\n",
        "  parse_all_documents('/content/drive/My Drive/MathIRData/ARQMathAgg/dataset_v2/',\n",
        "                      'collection_test.tsv', 'queries_test.tsv', 'aggregates/collection_agg_test.json')\n",
        "\n",
        "def get_top_k_paragraph(tokenizer, model, paragraphs, query, k):\n",
        "    if k > len(paragraphs):\n",
        "      raise ValueError(f\"k ({k}) cannot be greater than the number of paragraphs ({len(paragraphs)}).\")\n",
        "    # Turn latex into plaintext first\n",
        "    query = LatexNodes2Text(math_mode='verbatim', strict_latex_spaces=True).latex_to_text(query)\n",
        "    paragraphs = [(pid, LatexNodes2Text(math_mode='verbatim', strict_latex_spaces=True).latex_to_text(text)) for pid, text in paragraphs]\n",
        "\n",
        "    results = []\n",
        "    for id, (pid, paragraph) in enumerate(paragraphs):\n",
        "      inputs = tokenizer.encode_plus(query, paragraph, return_tensors=\"pt\", truncation=True)\n",
        "      with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        score = torch.nn.functional.softmax(outputs.logits, dim=-1)[0][1].item()\n",
        "      results.append((id, pid, score))\n",
        "\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "    rel_scores = [score for _, _, score in results]\n",
        "    top_k_results = results[:k]\n",
        "    return top_k_results, rel_scores\n",
        "\n",
        "def parse_all_documents(base_path, collection_name, queries_name, meta_name):\n",
        "    dirfiles = listdir(base_path)\n",
        "    print(dirfiles)\n",
        "\n",
        "    collection_path = join(base_path, collection_name)\n",
        "    queries_path = join(base_path, queries_name)\n",
        "    meta_path = join(base_path, meta_name)\n",
        "    qrelfile = '/content/drive/My Drive/MathIRData/ARQMathAgg/dataset_v2/qrel_test'\n",
        "    eval_res_out_path = f'/content/drive/My Drive/MathIRData/ARQMathAgg/Evaluation/ALBERT/'\n",
        "\n",
        "    k = 1\n",
        "\n",
        "    try:\n",
        "        with open(meta_path, 'r', encoding='utf-8') as file:\n",
        "            meta = json.load(file)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSONDecodeError: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    collection = {}\n",
        "    with open(collection_path, 'r', encoding='utf-8') as file:\n",
        "      for line in file:\n",
        "        match = re.match(r\"(\\d+)\\s+(.*)\", line)\n",
        "        if match:\n",
        "          index = int(match.group(1))\n",
        "          text = match.group(2).strip()\n",
        "          collection[index] = text\n",
        "\n",
        "\n",
        "    queries = {}\n",
        "    with open(queries_path, 'r', encoding='utf-8') as file:\n",
        "      for line in file:\n",
        "        match = re.match(r\"(\\d+)\\s+(.*)\", line)\n",
        "        if match:\n",
        "          index = int(match.group(1))\n",
        "          text = match.group(2).strip()\n",
        "          queries[index] = text\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"AnReu/albert-for-math-ar-base-ft\")\n",
        "\n",
        "    runfile = f'/content/drive/My Drive/MathIRData/ARQMathAgg/Evaluation/ALBERT/run'\n",
        "    with open(runfile, 'w', encoding='utf-8') as f:\n",
        "      for obj in meta:\n",
        "        qid = obj['qid']\n",
        "        pids = obj['pids']\n",
        "        correct_idx = obj['corr_idx']\n",
        "        paragraphs = [(pid, collection.get(pid)) for pid in pids]\n",
        "        query = queries.get(qid)\n",
        "        top_k, rel_scores = get_top_k_paragraph(tokenizer, model, paragraphs, query, k)\n",
        "        for idx, (id, pid, score) in enumerate(top_k):\n",
        "            f.write(f\"{qid} Q0 {pid} {idx+1} {score} STANDARD\\n\")\n",
        "\n",
        "\n",
        "    qrels = ir_measures.read_trec_qrels(qrelfile)\n",
        "    runs = ir_measures.read_trec_run(runfile)\n",
        "    cutoffs = [k]\n",
        "\n",
        "    ndcg_measure = [nDCG(cutoff=cutoff) for cutoff in cutoffs]\n",
        "    p_measure = [P(cutoff=cutoff) for cutoff in cutoffs]\n",
        "    jugded_measure = [Judged(cutoff=cutoff) for cutoff in cutoffs]\n",
        "    mrr_measure = [MRR(cutoff=cutoff) for cutoff in cutoffs]\n",
        "    map_measure = [MAP(cutoff=cutoff) for cutoff in cutoffs]\n",
        "\n",
        "    all_measures = list(itertools.chain(ndcg_measure, p_measure, jugded_measure, mrr_measure, map_measure))\n",
        "\n",
        "    print(\"Evaluating on:\")\n",
        "    print(all_measures)\n",
        "\n",
        "    eval_res = ir_measures.calc_aggregate(all_measures, qrels, runs)\n",
        "    with open(f\"{eval_res_out_path}/res.csv\", 'w', encoding='utf-8') as f:\n",
        "      f.write(\"Metric,Value\\n\")\n",
        "      for measure in all_measures:\n",
        "        f.write(f\"{str(measure)},{eval_res[measure]}\\n\")\n",
        "\n",
        "\n",
        "def parse_latex_into_array(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    latex_text = ''.join(lines)\n",
        "\n",
        "    text = LatexNodes2Text(math_mode='verbatim', strict_latex_spaces=True).latex_to_text(latex_text)\n",
        "    paragraphs = [para.strip() for para in text.split(\".\\n\\n\") if para.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}